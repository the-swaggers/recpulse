from typing import Any, Callable, Literal

import numpy as np
from pydantic import BaseModel, StrictStr, ValidationError, model_validator

import classes.activations as activations

PRECISIONS = float | np.float16 | np.float32 | np.float64
TENSOR_TYPE = np.ndarray[PRECISIONS, Any]

ACTIVATION_FUNCTIONS = Literal[
    "linear",
    "sigmoid",
    "softmax",
    "relu",
    "leaky_relu",
    "parametric_relu",
    "binary_step",
    "tanh",
    "arctan",
    "elu",
    "swish",
]

STR2ACTIVATION = {
    "linear": activations.linear,
    "sigmoid": activations.sigmoid,
    "softmax": activations.softmax,
    "relu": activations.relu,
    "leaky_relu": activations.leaky_relu,
    "parametric_relu": activations.parametric_relu,
    "binary_step": activations.binary_step,
    "tanh": activations.tanh,
    "arctan": activations.arctan,
    "elu": activations.elu,
    "swish": activations.swish,
}


class Dense(BaseModel):
    """Dense layer class.

    Layer that connects all inputs to all outputs and adds biases.
    """

    _output_shape: tuple
    _input_shape: tuple | None = None
    name: StrictStr | None = None
    _weights: np.ndarray | None = None
    _activation: Callable[[Any], TENSOR_TYPE] | None = None

    @model_validator(mode="after")  # type: ignore
    def validator(self) -> None:
        """Validate parameters of the layer."""
        output_size, _ = self._output_shape

        if output_size <= 0:
            raise ValidationError("Output shape must be greater than 0.")

        if self._input_shape is not None:
            input_size, _ = self._input_shape

            if input_size <= 0:
                raise ValidationError("Input shape must be greater than 0.")

    def __init__(
        self,
        output_shape: tuple,
        input_shape: tuple | None = None,
        activation: ACTIVATION_FUNCTIONS = "linear",
        name: StrictStr | None = None,
    ) -> None:
        super().__init__(
            _output_shape=output_shape,
            _input_shape=input_shape,
            name=name,
            _activation=STR2ACTIVATION[activation],
        )

        if input_shape is not None:
            self.initialize_weights()

    @property
    def input_shape(self) -> tuple | None:
        """Python getter."""
        return self._input_shape

    @input_shape.setter
    def input_shape(self, value: tuple):
        """Python setter."""
        size, _ = value
        if not isinstance(size, int):
            raise ValidationError(
                "Input size must be an integer or an integer" " in format of tuple of shape (1, )."
            )
        if size <= 0:
            raise ValueError("Input size must be positive.")
        self._input_shape = value

    @property
    def output_shape(self) -> tuple:
        """Python getter."""
        return self._output_shape

    def initialize_weights(
        self,
        input_shape: tuple | None = None,
        mean: float | None = None,
        standard_deviation: float | None = None,
    ) -> None:
        """Weights initializer.

        Creates a numpy array of shape (input_size + 1, self.output_size) with integers
        randomly generated by normal distribution.

        input_size + 1 is used instead of input_size to include biases for this layer.

        Inputs:
            input_size (int): size of the previous neuron layer.
            mean (float): mean of normal distribution for weights initialization
            standard_deviation (float): standard deviation of normal distribution
                 for weights initialization
        """

        if self._input_shape is None:
            if input_shape is None:
                raise ValueError(
                    "input_shape is not specified neither in the function execution, "
                    "nor in the class's instance"
                )
            self._input_shape = input_shape

        input_size, _ = self.input_shape  # type: ignore
        output_size, _ = self.output_shape

        if standard_deviation is None:
            standard_deviation = 1
        elif standard_deviation <= 0:
            raise ValueError("standard deviation must be greater than zero")

        if mean is None:
            mean = 0

        self._weights = np.random.normal(
            mean, standard_deviation, size=(input_size + 1, output_size)  # type: ignore
        )

    def propagate(self, inputs: np.ndarray) -> np.ndarray:
        """Propagates input tensor.

        Inputs:
            inputs (np.array): an array of shape (self.input_size, )

        Returns:
            np.array: an array of shape (self.output_size, )
        """

        if self._weights is None:
            raise ValueError("Weights aren't initialized!")
        if self._activation is None:
            raise ValueError("Activation function isn't initialized!")

        # add 1 so that the bias doesn't need any additional code apart from matmul
        modified_input = np.append(inputs, 1, axis=0)

        # get new layer
        propagated = np.matmul(modified_input, self._weights)

        return self._activation(propagated)
